{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Routing Between Capsules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capsule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A capsule is a group of neurons whose activity vectors represent various instantiation parameters of an object.\n",
    "- The whole process\n",
    "    - Active capsules at the lower level(triangle, rectangle) makes prediction for the instantiation parameters of higher level capsules(boat, house).\n",
    "    - When multiple capsules of a lower level come to an agreement, the capsules at higher level become active.\n",
    "    - The probability of presence of entity is determined by the length of the activity vectors and the vector's orientation tells about the properties of an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist:\n",
    "    def __init__(self, batch_size):\n",
    "        dataset_transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "\n",
    "        train_dataset = datasets.MNIST('./mnist', train=True, download=True, transform=dataset_transform)\n",
    "        test_dataset = datasets.MNIST('./mnist', train=False, download=True, transform=dataset_transform)\n",
    "        \n",
    "        self.train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary Capsule layer\n",
    "- In this layer, we replace the scaler-output feature detector of CNN with 8-dim vector output capsule.\n",
    "- The primary capsules are the lowest level of entities.\n",
    "- Equivarience: translation of input features results in an equivalent translation of outputs. \n",
    "    - Traditional CNN fails to encode these feature due to the nature of scalar-output feature detector and pooling layers.\n",
    "- This layer can be designed using several Convolution layer. In the paper, authors used a stack of 8 Convolution layer each with 32 feature maps, kernel size of 9x9, stride 2 and zero padding. We pass the output of the first convolution through every convolution in this layer and our expected final output is [batch_size,primary_num_capsule,primary_capsule_dim]. So we need to reshape the initial output shape to get our expected shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **squash function**\n",
    "    - They use non-linear activation, squashing function, so that the length of output vector becomes less than 1 because it is used to represent probability.\n",
    "$$v_{j} = \\dfrac{||s_j||^2}{1 + ||s_j||^2} \\dfrac{s_j}{||s_j||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=2, padding=0) \n",
    "                          for _ in range(num_capsules)])\n",
    "            \n",
    "    def forward(self, x):      \n",
    "        u = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]   \n",
    "        u = torch.cat(u, dim=-1)\n",
    "        \n",
    "        return self.squash(u)  # [128, 1152, 8]\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)  # 8-dim vector의 norm이 1보다 작아지게 squashing\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Routing By agreement\n",
    "- When the capsules at the lowel level come upon an agreement over the presence of an entity at higher level, only the capsule corresponding to that entity becomes active. Hence, other capsules are rejected and this is instrumental in **removing noise** from the model. \n",
    "- Therefore, when there is a strong agreement between the capsules at the lower levels for the activity of the capsules at the higher level, the capsules at the lower level are **routed** to the capsules at higher level.\n",
    "- In MNIST case, \n",
    "    - Every capsule in the Primary layer tries to predict the output of every capsule in Digit layer.\n",
    "    - 자신의 prediction과 일치하는(scalar product가 큰) higher level capsule에게 자신의 output을 전달.\n",
    "    - Using the output vector of primary capsule we will predict the output vector of digit capsule by fully-connected layer.\n",
    "    - we need a matrix, $W_{ij}$ for each pair of capsule in primary and digit layer. primary capsule 8D, digit capsule 16D, so $W_{ij}$ is $8 \\times 16$\n",
    "    - **prediction vector**: $\\hat{u}_{j|i} = W_{ij}u_{i}$\n",
    "    - 각 1152개 prime caps가 10개 digit caps에 대해 모두 개별 예측을 한 것. \n",
    "    - Compute the weighted sum of all the ‘predicted output vectors’ $\\hat{u}_{j|i}$ for each digit capsule. $s_j = \\sum{c_{ij}\\hat{u}_{j|i}}$\n",
    "    - The output of digit capsule $s_j$ might give vectors larger than 1, so we need to squash these 16 dimensional vector by $v_j = squash(s_j)$.\n",
    "    - Now we need to determine which primary capsule agrees with which digit capsule. We just simply compute it by the scalar product of each instance between predicted vector by primary capsule. (u_hat, v_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=10, num_routes=32 * 6 * 6, in_channels=8, out_channels=16):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
    "        # (1152,10) 간의 모든 pair에 대해 8x16 matrix를 갖고 있어야 한다. \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "        # x.shape : [128, 1152, 10, 8, 1]\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        # W.shape : [128, 1152, 10, 16, 8]\n",
    "        u_hat = torch.matmul(W, x)\n",
    "        # u_hat.shape: [128, 1152, 10, 16, 1]\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n",
    "        # b_ij.shape: [1, 1152, 10, 1]        \n",
    "        b_ij = b_ij.to(device)\n",
    "\n",
    "        num_iterations = 3\n",
    "        for iteration in range(num_iterations):\n",
    "            c_ij = F.softmax(b_ij)\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = self.squash(s_j)\n",
    "            \n",
    "            if iteration < num_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "- Decoder part consist of three Fully Connected layers. It took digit capsules output as input and reconstruct the same input image.\n",
    "- During training, instead of using all the 16 dimensinal digit capsule they only use the capsule corresponds to the target digit and masked out other capsules to reconstruct the input image. These 16 dimensional digit capsules are feed into a decoder consisting of 3 fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.reconstraction_layers = nn.Sequential(\n",
    "            nn.Linear(16 * 10, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, data):\n",
    "        classes = torch.sqrt((x ** 2).sum(2))\n",
    "        classes = F.softmax(classes)\n",
    "        \n",
    "        _, max_length_indices = classes.max(dim=1)\n",
    "        masked = Variable(torch.sparse.torch.eye(10))\n",
    "        \n",
    "        masked = masked.to(device)\n",
    "        masked = masked.index_select(dim=0, index=max_length_indices.squeeze(1).data)\n",
    "        \n",
    "        reconstructions = self.reconstraction_layers((x * masked[:, :, None, None]).view(x.size(0), -1))\n",
    "        reconstructions = reconstructions.view(-1, 1, 28, 28)\n",
    "        \n",
    "        return reconstructions, masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv_layer = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9)\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_layer(x))\n",
    "        output = self.digit_capsules(self.primary_capsules(x))\n",
    "        reconstructions, masked = self.decoder(output, data)\n",
    "        return output, reconstructions, masked\n",
    "    \n",
    "    def loss(self, data, x, target, reconstructions):\n",
    "        return self.margin_loss(x, target) + self.reconstruction_loss(data, reconstructions)\n",
    "    \n",
    "    def margin_loss(self, x, labels, size_average=True):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
    "\n",
    "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
    "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
    "\n",
    "        loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def reconstruction_loss(self, data, reconstructions):\n",
    "        loss = self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n",
    "        return loss * 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsule_net = CapsNet().to(device)\n",
    "optimizer = Adam(capsule_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "mnist = Mnist(batch_size)\n",
    "\n",
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parkchanwoo/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/parkchanwoo/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.0625\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.9296875\n",
      "train accuracy: 1.0\n",
      "0.2779360364542714\n",
      "test accuracy: 0.9921875\n",
      "0.06234931467761156\n",
      "train accuracy: 0.9609375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 1.0\n",
      "0.0517857832345627\n",
      "test accuracy: 0.9921875\n",
      "0.04099987896713369\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    capsule_net.train()\n",
    "    train_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(mnist.train_loader):\n",
    "\n",
    "        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"train accuracy:\", sum(np.argmax(masked.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "    print(train_loss / len(mnist.train_loader))\n",
    "        \n",
    "    capsule_net.eval()\n",
    "    test_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(mnist.test_loader):\n",
    "\n",
    "        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"test accuracy:\", sum(np.argmax(masked.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "    \n",
    "    print(test_loss / len(mnist.test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images_separately(images):\n",
    "    \"Plot the six MNIST images separately.\"\n",
    "    fig = plt.figure()\n",
    "    for j in range(1, 7):\n",
    "        ax = fig.add_subplot(1, 6, j)\n",
    "        ax.matshow(images[j-1], cmap = matplotlib.cm.binary)\n",
    "        plt.xticks(np.array([]))\n",
    "        plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images_separately(data[:6,0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images_separately(reconstructions[:6,0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
